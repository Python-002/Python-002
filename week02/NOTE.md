# 学习笔记 week02

还是格言式为主吧，每节课记几句，其实看的不太系统，没有大块儿时间踏实儿仔细地看的，花插着看的

按每节课来吧

# 异常捕获预处理

+ python的异常trace 打印和Java是反着的？课程里面也说了，这个和直觉相反，不知道python这个设计怎么想的，估计是觉得直接tail log到最后所以这样看着方便？
+ 可能资源连接里的pretty_errors 都没印象在课程里提到了，可见看的多么的不认真
+ try...except...finally，我目前理解就是它叫except，而不是catch，从动作上挺传神的，主动的去匹配异常？而不是被动等待捕获？
+ Java叫throw，python是叫raise
+ with有点像try resource，上下文管理器对应的Closeable接口？存疑，后面还得再看

# MySQL

## MySQL本身

+ 安装个开发目的的很熟悉了，比如centos上rpm的，还有docker的，所以跳过，我是有现成的？当然没有，因此趁着有张券，买了一个实例
+ 阿里云4块钱一年买了个1c1g的，以后的学习专用吧

## MySQL Python 库

+ 选的PyMySQL
  - 课程里说是更稳定？
  - 但好像有地方看到说，是因为它安装最容易，因为纯python的
    * 安装就是```pip install pymysql```
+ 配置文件的部分...没听明白，不应该对接环境变量么？（12 factor）
  - 有[这么个](https://juejin.im/post/6844904079496314894) 地方可以参考，里面提到一个[库](https://pypi.org/project/environs/)
  - 后面再看这个吧

+ 好像哪里听说过有个Django的兼容和速度会更好一点
+ 课程里面提到Python的方法签名？是类型提示 typo hint？不是类型定义
+ 最后还是忘了调了用户名密码这个，回头再改吧，我这可还是真正的资源...

## 反爬虫：模拟浏览器头部信息

其实讲的是反反爬虫，后面几个其实都是

+ User-Agent不是太难理解的内容
+ 介绍了一个库吧就是
+ 作业里试图用这个改进一下，但是...反正没有打log看效果，maoyan经常需要去校验一下，估计还是cookie+ ip检查之类
+ 作业没对这里提要求，所以没太认真，我有罪...

## 反爬虫：cookies验证

+ 老实说又是作业没要求，所以看的也极不认真，应该回过头来看下
+ httpbin，还好吧，对于不熟悉一http各类调试工具的同学可能有用，但也不失为是一个确实做得不错的echo server吧

## 反爬虫：使用WebDriver模拟浏览器行为

+ 这个虽然作业算是有要求，刚好带测试组走过一遍selenium初步，所以这个是跳着看的
+ 例子里面等待元素可取是sleep，嗯，其实wait until是正解，不过说明确实get不是阻塞的，那天博哥演示时候玩儿花了应该确实是网络问题
+ 要不是给了个w3c的webdriver的连接，我都差点忘了这东西其实算是中立的


## 反爬虫：验证码识别

+ 一样，又是作业没要求，所以看的也极不认真，应该回过头来看下
+ 这里是个tessdata库的问题，确实看的操，好多同学群里问时候我都不知道大家说的是啥

## 爬虫中间件&系统代理IP

+ 其实我觉得系统代理这个还好，单纯的正向代理目的，就是一个环境变量问题，和12 factor不冲突
+ 但是就怕一个ip也搞不定，所以还是搞个配置文件来比较好，爬虫的世界，export差点儿

## 自定义中间件&随机代理IP

+ 这个因为跟作业关联性非常大...但是看的也没多仔细
+ 代码拿过来直接用了，稍微改了改
+ 代理ip这东西，我觉得还得有一个预先筛选检查的工具，还得定期更新，否则太特么慢了
+ 这个代理ip的可用性方面的不确定，倒是直接和异常有关
  - 但是感觉并不是try/except这个路数，而是scrapy.Request的errback的路数吧，然后自旋重试，甚至去掉代理来让特定请求尽可能成功完成一次应该是一个工程上可行的方法
  - 时间有限，作业里就没调这个思路
+ 作业里的一些坑和tip，这里记录一下
  - response对象可以取到request，request里面有个meta可以查看proxy设置的情况
  - https是不是使用http代理，这个其实是自己实现的那个中间件来完成的，没时间，所以没有修改过多原来的课程参考代码

## 分布式爬虫

+ 就是redis做中间缓存和辅助多节点协调调度
+ redis的python库应该亲手摆弄一下才对，但是，我没有，毕竟作业无关，就低优先级了
+ 课程看的很糙吧，属于挂机刷分似的，这周可能有点开始有点松懈了

## 本周作业

### 作业一

+ 增加代理ip的，算是完成了吧，实测确实走的代理，我觉得备注提示的github免费ip库倒是有用，我自己搜到的两个我都star收藏了...其实可能也真的没什么用
+ MySQL和CSV都保留了，做的也很初级，我连库表主键都没建，本来做之前还想翻一个snowflake的库来着，算了，劝自己抓主要矛盾了
+ 异常的没搞懂怎么回事，这里列一下吧，省的返回第一课那里更混乱
  - 我确实最后觉的题目的“下载部分增加异常捕获和处理机制”有点问题
    * 首先是scrapy框架的异常捕获应该是靠框架的（解析过程中的不算，但是下载请求部分我认为功能是框架提供，无论是设计上还是实际看api，例如errback确实是框架来做了）
    * 其次是处理机制，课程第一讲有点小粗糙，泛泛的说了一些异常的东西，不太容易直接联系到作业上
    * 还是处理机制，处理机制其实根据具体异常的原因，不尽相同，这个没有一定的开发经验确实不好把握，我勉强这里总结一下
    * 处理方式一：最常用的，是快速失败，更多是为了在迭代开发阶段发现问题，或生产时断路而不引发连锁反应，典型是Java明确的非受控异常
    * 处理方式二：有限次重试和审计用log记录，目的更多的是保证业务上最终的一致性
    * 处理方式三：就是记录log，这个通常是因为...自己也没想清楚到底应该怎么样
    * 最后，扯个可能没用的，我以前觉得java分受控非受控异常有点罗嗦的，目前看起来，确实java的语言设计者比python的可能更懂那么一点工程问题
  - 上述方式一：这实际上就是scrapy框架自己在做的，另外学会raise，可能比假装try expect更有价值
  - 上述方式二：时间的因素干扰，没有做，思路前面好像说过了，就是跟代理ip有关，它如果失效了，设计一个重试的路径，最后退化一个直连来尽可能的处理一次数据
  - 上述方式三：反正作业里有那么一丢丢

  ### 作业二

  + 按说我应该把重点放在request上，但是最后要assert的东西比较多吧，主要是获取cookie
  + selenium模拟登录吧，IDE录制出品，调通，交作业
